\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{fancyhdr}

% Geometry settings to match the readable style of provided PDFs
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% Header Setup
\pagestyle{fancy}
\fancyhf{}
\rhead{\textbf{Pointer Analysis}}
\lhead{Oral Exam Script}
\cfoot{\thepage}

\title{\textbf{Pointer Analysis: Inclusion-based \& Unification-based}\\ \large (Target: 15 Minutes)}
\author{}
\date{}

\begin{document}

\maketitle

\section{1. Introduction \& The "Why" (3 Minutes)}

Today I will present \textbf{Pointer Analysis}, specifically focusing on the inclusion-based techniques described in Chapter 11. 

To start, I want to clarify \textit{why} this is such a fundamental problem in compiler construction. It all comes down to one word: \textbf{Safety}. When a compiler tries to optimize code—say, by keeping a variable in a register or removing dead code—it must guarantee that the program's behavior doesn't change. Pointers threaten this guarantee because they introduce \textbf{Aliasing}.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item \textbf{Header:} Motivation: Aliasing
    \item \textbf{Code Example:}
\begin{verbatim}
x = 10;
*p = 5;      // The "Unknown" Store
y = x;
// Can we optimize this to y = 10?
\end{verbatim}
\end{itemize}
\vspace{0.5cm}

Consider this snippet. We assign 10 to $x$. Then we store 5 into the address $p$. Finally, we read $x$. A naive compiler might look at line 3 and say: "Well, $x$ is 10, so let's just replace $y = x$ with $y = 10$."

\textbf{But can we do that?}
If $p$ happens to point to $x$, then line 2 overwrites $x$ with 5. If we optimized $y$ to 10, we would have broken the program.

\begin{itemize}
    \item \textbf{The Goal:} Compute a set of abstract locations that $p$ might point to.
    \item \textbf{The Optimization:} If $x$ is NOT in that set, we can optimize safely. If $x$ IS in that set, we must be conservative.
\end{itemize}

\section{2. Concepts: Locations \& Constraints (3 Minutes)}

Before we look at the algorithms, we need to define our \textbf{Domain}. What are we analyzing? We model memory as a set of \textbf{Abstract Locations}. Obviously, every global and local variable (like $x$ or $y$) is a location. But we also need to handle dynamic memory (malloc).

Since a program can call `malloc` infinite times in a loop, we cannot track every single block. Instead, we group all blocks allocated at a specific line of code into one \textbf{Abstract Object}.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item List the Abstract Locations:
    \begin{enumerate}
        \item Variables ($x, y, z$)
        \item Allocation Sites ($S_1, S_2 \dots$)
    \end{enumerate}
    \item Define the goal: Compute $pt(p)$, the subset of locations $p$ may point to.
\end{itemize}

\section{3. Andersen’s Analysis (Inclusion-based) (6 Minutes)}

The most common solution is \textbf{Andersen’s Analysis}. This relies on \textbf{Subset Constraints}. The intuition is that assignments create a flow of data. If I say $p = q$, I am saying that $p$ can now see everything $q$ sees.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item Draw the Rules Table:
\end{itemize}
\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{l | l | l}
    \textbf{Stmt} & \textbf{Code} & \textbf{Constraint} \\
    \hline
    Address & $p = \&x$ & $\{x\} \subseteq pt(p)$ \\
    Copy & $p = q$ & $pt(q) \subseteq pt(p)$ \\
    Load & $p = *q$ & $\forall v \in pt(q): pt(v) \subseteq pt(p)$ \\
    Store & $*p = q$ & $\forall v \in pt(p): pt(q) \subseteq pt(v)$ \\
\end{tabular}
\end{center}
\vspace{0.5cm}

The first two rules are simple. The complexity lies in the Load and Store rules. 
\begin{itemize}
    \item Take $p = *q$. We are dereferencing $q$.
    \item Since $q$ holds addresses, we must look at every location $v$ that $q$ \textit{might} point to.
    \item For each of those $v$'s, we copy their contents into $p$.
\end{itemize}
This is dynamic. As our knowledge of $pt(q)$ grows, the constraints generated by this rule also grow.

\subsection*{The Algorithm: Constraint Graph}

We solve this using a \textbf{Constraint Graph}. Nodes are variables, and an edge from $q \to p$ means $pt(q) \subseteq pt(p)$.

\vspace{0.5cm}
\noindent \textbf{Whiteboard Action:}
\begin{itemize}
    \item Draw 4 nodes in a diamond shape: $p, q, x, y$.
    \item Illustrate the Worklist approach:
    \begin{enumerate}
        \item Start with base constraints (e.g., $p = \&x \rightarrow$ add $x$ to $pt(p)$).
        \item If $pt(p)$ changes, propagate new info to neighbors via outgoing edges.
        \item Repeat until Fixed Point.
    \end{enumerate}
\end{itemize}
\vspace{0.5cm}

\textbf{Complexity:} $O(N^3)$. This is due to the dynamic Load/Store rules. A change in one pointer set can trigger a ripple effect that adds edges to the graph, triggering more propagation.

\section{4. Steensgaard’s Analysis (Unification) (3 Minutes)}

$O(N^3)$ is acceptable for small programs, but too slow for massive codebases (like the Linux Kernel). This brings us to \textbf{Steensgaard’s Analysis}. 

Steensgaard asked: "What if we sacrifice precision for speed?"
Instead of allowing data to flow in one direction ($q \subseteq p$), Steensgaard forces the sets to be identical ($q = p$). If we assign $p = q$, we treat them as the \textbf{same node} in the graph.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item \textbf{Steensgaard:} Unification ($=$) not Subset ($\subseteq$).
    \item Draw the Rules Table for Comparison:
\end{itemize}
\begin{center}
\begin{tabular}{l | l | l}
    \textbf{Stmt} & \textbf{Code} & \textbf{Constraint (Unification)} \\
    \hline
    Address & $p = \&x$ & $pt(p) = \{x\}$ (Merge $x$ into $p$'s points-to set) \\
    Copy & $p = q$ & $pt(p) = pt(q)$ (Merge sets $p$ and $q$) \\
    Load & $p = *q$ & $pt(p) = *pt(q)$ (Merge $p$ with whatever $q$ points to) \\
    Store & $*p = q$ & $*pt(p) = pt(q)$ (Merge whatever $p$ points to with $q$) \\
\end{tabular}
\end{center}
\vspace{0.5cm}

\textbf{Key Difference in Logic:}
\begin{itemize}
    \item In Andersen's, if $p$ points to $\{x, y\}$, the constraint $*p = q$ means "Updates to $*p$ flow into $x$ \textbf{AND} $y$."
    \item In Steensgaard's, we cannot handle "AND." We can only handle one set. So, if $p$ points to $x$ and $y$, Steensgaard \textbf{unifies} $x$ and $y$. They become the same node in the graph.
\end{itemize}

\vspace{0.5cm}
\noindent \textbf{Whiteboard Action:}
\begin{itemize}
    \item \textbf{Diagram:}
    \begin{itemize}
        \item \textbf{Andersen:} Node $p$ has two outgoing arrows to $x$ and $y$.
        \item \textbf{Steensgaard:} Nodes $x$ and $y$ are merged into a single blob. Node $p$ has one arrow to that blob.
    \end{itemize}
\end{itemize}
\vspace{0.5cm}

This turns the problem from Graph Reachability into \textbf{Union-Find}.
\begin{itemize}
    \item \textbf{Complexity:} Almost Linear ($O(N \alpha(N))$).
    \item \textbf{Downside:} \textbf{False Positives}. Because we merged $x$ and $y$, the analysis might report that a pointer intended for $x$ also points to $y$. This is a loss of precision.
\end{itemize}

\section{5. Conclusion (\textless 1 Minute)}

\textbf{Summary:}
\begin{itemize}
    \item We defined \textbf{Aliasing} as the core problem preventing optimization.
    \item We explored \textbf{Andersen’s method} (Inclusion-based), which is precise ($O(N^3)$) and uses a constraint graph.
    \item We contrasted it with \textbf{Steensgaard’s method} (Unification-based), which is fast ($O(N \alpha(N))$) but less precise due to merging.
\end{itemize}

In modern compilers (LLVM/GCC), we typically use inclusion-based analysis heavily optimized with \textbf{Cycle Detection} to ensure scalability.

\newpage

\section*{Appendix: Quick Reference}

\subsection*{Comparison Table}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Analysis} & \textbf{Constraint} & \textbf{Complexity} & \textbf{Data Structure} \\
\hline
Andersen & Subset ($\subseteq$) & $O(n^3)$ & Constraint Graph \\
\hline
Steensgaard & Equality ($=$) & $O(n \alpha(n))$ & Union-Find \\
\hline
\end{tabular}
\end{center}

\subsection*{Key Terminology}
\begin{itemize}
    \item \textbf{Abstract Location:} Represents a variable or a `malloc` site.
    \item \textbf{Flow-Insensitive:} The order of statements doesn't matter (sets accumulate).
    \item \textbf{Fixed Point:} When propagating sets produces no new changes.
    \item \textbf{Cycle Detection:} Optimization for Andersen's (all nodes in a cycle are merged).
\end{itemize}

\end{document}