\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{stmaryrd}

% Geometry settings to match the readable style of provided PDFs
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% Header Setup
\pagestyle{fancy}
\fancyhf{}
\rhead{\textbf{Pointer Analysis}}
\lhead{Oral Exam Script}
\cfoot{\thepage}

\title{\textbf{Pointer Analysis: Inclusion-based \& Unification-based}\\ \large (Target: 15 Minutes)}
\author{}
\date{}

\begin{document}

\maketitle

\section{1. Introduction \& The "Why" (3 Minutes)}

Today I will present \textbf{Pointer Analysis}, specifically focusing on the inclusion-based techniques described in Chapter 11. 

To start, I want to clarify \textit{why} this is such a fundamental problem in compiler construction. It all comes down to one word: \textbf{Safety}. When a compiler tries to optimize code—say, by keeping a variable in a register or removing dead code—it must guarantee that the program's behavior doesn't change. Pointers threaten this guarantee because they introduce \textbf{Aliasing}.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item \textbf{Header:} Motivation: Aliasing
    \item \textbf{Code Example:}
\begin{verbatim}
x = 10;
*p = 5;      // The "Unknown" Store
y = x;
// Can we optimize this to y = 10?
\end{verbatim}
\end{itemize}
\vspace{0.5cm}

Consider this snippet. We assign 10 to $x$. Then we store 5 into the address $p$. Finally, we read $x$. A naive compiler might look at line 3 and say: "Well, $x$ is 10, so let's just replace $y = x$ with $y = 10$."

\textbf{But can we do that?}
If $p$ happens to point to $x$, then line 2 overwrites $x$ with 5. If we optimized $y$ to 10, we would have broken the program.

\begin{itemize}
    \item \textbf{The Goal:} Compute a set of abstract locations that $p$ might point to.
    \item \textbf{The Optimization:} If $x$ is NOT in that set, we can optimize safely. If $x$ IS in that set, we must be conservative.
\end{itemize}

\section{2. Concepts: Locations \& Constraints (3 Minutes)}

Before we look at the algorithms, we need to define our \textbf{Domain}. What are we analyzing? We model memory as a set of \textbf{Abstract Locations}. Obviously, every global and local variable (like $x$ or $y$) is a location. But we also need to handle dynamic memory (malloc).

Since a program can call `malloc` infinite times in a loop, we cannot track every single block. Instead, we group all blocks allocated at a specific line of code into one \textbf{Abstract Object}.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item List the Abstract Locations:
    \begin{enumerate}
        \item Variables ($x, y, z$)
        \item Allocation Sites ($S_1, S_2 \dots$)
    \end{enumerate}
    \item Define the goal: Compute $pt(p)$, the subset of locations $p$ may point to.
\end{itemize}

\section{3. Andersen’s Analysis (Inclusion-based) (6 Minutes)}

The most common solution is \textbf{Andersen’s Analysis} (Section 11.2). This relies on \textbf{Subset Constraints} (inclusion). The intuition is that assignments create a flow of data. If I say $p = q$, I am saying that $p$ can now point to everything $q$ points to.

\vspace{0.5cm}
\noindent \textbf{Write on Whiteboard:}
\begin{itemize}
    \item \textbf{Domain:} $pt(p) = \llbracket p \rrbracket$ is the set of abstract cells $p$ may point to.
    \item \textbf{Rules:}
\end{itemize}
\renewcommand{\arraystretch}{1.5}
\begin{center}
\begin{tabular}{l | l | l}
    \textbf{Stmt} & \textbf{Code} & \textbf{Constraint} \\
    \hline
    Allocation & $X = \text{alloc } P$ & $\text{alloc-}i \in \llbracket X \rrbracket$ \\
    Address & $X_1 = \&X_2$ & $X_2 \in \llbracket X_1 \rrbracket$ \\
    Copy & $X_1 = X_2$ & $\llbracket X_2 \rrbracket \subseteq \llbracket X_1 \rrbracket$ \\
    Load & $X_1 = *X_2$ & $\forall c \in \text{Cell}: c \in \llbracket X_2 \rrbracket \Rightarrow \llbracket c \rrbracket \subseteq \llbracket X_1 \rrbracket$ \\
    Store & $*X_1 = X_2$ & $\forall c \in \text{Cell}: c \in \llbracket X_1 \rrbracket \Rightarrow \llbracket X_2 \rrbracket \subseteq \llbracket c \rrbracket$ \\
\end{tabular}
\end{center}
\vspace{0.5cm}

The complexity lies in the Load and Store rules. They rely on the points-to set of the pointer being dereferenced. As our knowledge of $\llbracket X_2 \rrbracket$ grows, we discover new constraints (dynamic edges in the constraint graph).
\begin{itemize}
    \item \textbf{Complexity:} $O(N^3)$ in the worst case.
\end{itemize}

\section{4. Steensgaard’s Analysis (Unification-based) (3 Minutes)}

Alternatively, we have \textbf{Steensgaard’s Analysis} (Section 11.3). This is coarser but faster. Instead of subsets ($\subseteq$), it uses \textbf{Unification} ($=$) and equivalence classes. It treats assignments as bidirectional.

\vspace{0.5cm}
\noindent \textbf{Notation:}
\begin{itemize}
    \item $\llbracket X \rrbracket$ is a \textbf{term variable} representing the abstract cell that $X$ points to.
    \item We use a constructor, let's call it $\uparrow$ (or $t$), to represent ``pointer to''.
\end{itemize}

\begin{center}
\begin{tabular}{l | l | l}
    \textbf{Stmt} & \textbf{Code} & \textbf{Constraint} \\
    \hline
    Allocation & $X = \text{alloc } P$ & $\llbracket X \rrbracket = \uparrow \llbracket \text{alloc-}i \rrbracket$ \\
    Address & $X_1 = \&X_2$ & $\llbracket X_1 \rrbracket = \uparrow \llbracket X_2 \rrbracket$ \\
    Copy & $X_1 = X_2$ & $\llbracket X_1 \rrbracket = \llbracket X_2 \rrbracket$ \\
    Load & $X_1 = *X_2$ & $\llbracket X_2 \rrbracket = \uparrow \alpha \wedge \llbracket X_1 \rrbracket = \alpha$ \\
    Store & $*X_1 = X_2$ & $\llbracket X_1 \rrbracket = \uparrow \alpha \wedge \llbracket X_2 \rrbracket = \alpha$ \\
\end{tabular}
\end{center}

\vspace{0.5cm}
\textbf{Key Difference:}
\begin{itemize}
    \item In Andersen's (Copy): $X_1 = X_2 \implies \llbracket X_2 \rrbracket \subseteq \llbracket X_1 \rrbracket$. (Flow is directional).
    \item In Steensgaard's (Copy): $X_1 = X_2 \implies \llbracket X_1 \rrbracket = \llbracket X_2 \rrbracket$. (Flow is bidirectional/unified).
\end{itemize}
This unification merges the sets of locations pointed to by $X_1$ and $X_2$, losing precision but running in almost linear time $O(N \alpha(N))$.

\section{5. Conclusion (\textless 1 Minute)}

\textbf{Summary:}
\begin{itemize}
    \item We defined \textbf{Aliasing} as the core problem preventing optimization.
    \item We explored \textbf{Andersen’s method} (Inclusion-based), which is precise ($O(N^3)$) and uses a constraint graph.
    \item We contrasted it with \textbf{Steensgaard’s method} (Unification-based), which is fast ($O(N \alpha(N))$) but less precise due to merging.
\end{itemize}

In modern compilers (LLVM/GCC), we typically use inclusion-based analysis heavily optimized with \textbf{Cycle Detection} to ensure scalability.

\newpage

\section*{Appendix: Quick Reference}

\subsection*{Comparison Table}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Analysis} & \textbf{Constraint} & \textbf{Complexity} & \textbf{Data Structure} \\
\hline
Andersen & Subset ($\subseteq$) & $O(n^3)$ & Constraint Graph \\
\hline
Steensgaard & Equality ($=$) & $O(n \alpha(n))$ & Union-Find \\
\hline
\end{tabular}
\end{center}

\subsection*{Key Terminology}
\begin{itemize}
    \item \textbf{Abstract Location:} Represents a variable or a `malloc` site.
    \item \textbf{Flow-Insensitive:} The order of statements doesn't matter (sets accumulate).
    \item \textbf{Fixed Point:} When propagating sets produces no new changes.
    \item \textbf{Cycle Detection:} Optimization for Andersen's (all nodes in a cycle are merged).
\end{itemize}

\end{document}