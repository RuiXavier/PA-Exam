\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

% Geometry settings for readable script
\geometry{top=2cm, bottom=2cm, left=2.5cm, right=2.5cm}

% Custom Colors
\definecolor{boardcolor}{RGB}{240, 248, 255} % Light Alice Blue
\definecolor{actioncolor}{RGB}{255, 240, 240} % Light Red/Pink

% Custom Environments
\newtcolorbox{board}[1][]{
  colback=boardcolor,
  colframe=blue!30!black,
  title=\textbf{\textsc{Write on Board:}},
  fonttitle=\bfseries,
  #1
}

\newtcolorbox{action}[1][]{
  colback=actioncolor,
  colframe=red!30!black,
  title=\textbf{\textsc{Action:}},
  fonttitle=\bfseries,
  #1
}

\newcommand{\speech}[1]{
    \vspace{0.3cm}
    \noindent \textbf{Speech:} ``#1''
    \vspace{0.3cm}
}

\title{\textbf{Expanded Oral Exam Script: Pointer Analysis}\\ \large (Target: 15 Minutes)}
\author{}
\date{}

\begin{document}

\maketitle

\section{1. Introduction \& The "Why" (3 Minutes)}

\begin{action}
    Stand confidently. Don't write yet.
\end{action}

\speech{Good morning. Today I will present \textbf{Pointer Analysis}, specifically focusing on the inclusion-based techniques described in Chapter 11.

To start, I want to clarify \textit{why} this is such a fundamental problem in compiler construction. It all comes down to one word: \textbf{Safety}.

When a compiler tries to optimize code—say, by keeping a variable in a register or removing dead code—it must guarantee that the program's behavior doesn't change. Pointers threaten this guarantee because they introduce \textbf{Aliasing}.}

\begin{action}
    Turn to the board. Write the header ``Motivation: Aliasing''.
    Write this specific code example (Constant Propagation).
\end{action}

\begin{board}
\begin{verbatim}
// Motivation
x = 10;
*p = 5;      // The "Unknown" Store
y = x;       // Can we optimize this to y = 10?
\end{verbatim}
\end{board}

\speech{Consider this snippet. We assign 10 to `x`. Then we store 5 into the address `p`. Finally, we read `x`.
A naive compiler might look at line 3 and say: ``Well, `x` is 10, so let's just replace `y = x` with `y = 10`.''

\textbf{But can we do that?}
If `p` happens to point to `x`, then line 2 overwrites `x` with 5. If we optimized `y` to 10, we would have broken the program.

So, the goal of pointer analysis is to compute a set of abstract locations that `p` might point to. If `x` is NOT in that set, we can optimize safely. If `x` IS in that set, we must be conservative.}

\section{2. Concepts: Locations \& Constraints (3 Minutes)}

\speech{Before we look at the algorithms, we need to define our \textbf{Domain}. What are we analyzing?

We model memory as a set of \textbf{Abstract Locations}.
Obviously, every global and local variable (like `x` or `y`) is a location.
But what about dynamic memory? Malloc?}

\begin{action}
    Write ``Abstract Locations'' on the board.
\end{action}

\begin{board}
    1. Variables ($x, y, z$) \\
    2. Allocation Sites ($S_1, S_2 \dots$)
\end{board}

\speech{Since a program can call `malloc` infinite times in a loop, we cannot track every single block. Instead, we group all blocks allocated at a specific line of code into one \textbf{Abstract Object}.

With this domain, we can define our problem: For every variable $p$, we want to compute $pt(p)$, which is the subset of locations $p$ may point to.}

\section{3. Andersen’s Analysis (Inclusion-based) (6 Minutes)}

\speech{Now, let's look at the most common solution: \textbf{Andersen’s Analysis}.
This relies on \textbf{Subset Constraints}. The intuition is that assignments create a flow of data. If I say `p = q`, I am saying that `p` can now see everything `q` sees.}

\begin{action}
    Draw the rules table slowly. This takes time, so explain as you write.
\end{action}

\begin{board}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l | l | l}
    \textbf{Stmt} & \textbf{Code} & \textbf{Constraint} \\
    \hline
    Address & $p = \&x$ & $\{x\} \subseteq pt(p)$ \\
    Copy & $p = q$ & $pt(q) \subseteq pt(p)$ \\
    Load & $p = *q$ & $\forall v \in pt(q): pt(v) \subseteq pt(p)$ \\
    Store & $*p = q$ & $\forall v \in pt(p): pt(q) \subseteq pt(v)$ \\
\end{tabular}
\end{board}

\speech{(While writing Load/Store): The first two rules are simple. The last two are where the complexity lies.
Take the Load rule: `p = *q`.
We are dereferencing `q`. But `q` is a variable; it doesn't hold a value, it holds \textit{addresses}.
So we must look at every location `v` that `q` \textit{might} point to. For each of those `v`s, we copy their contents into `p`.

This is dynamic! As our knowledge of `pt(q)` grows, the constraints generated by this rule also grow.}

\subsection*{The Algorithm: Constraint Graph}

\speech{So, how do we actually solve this? We don't just stare at equations. The compiler builds a \textbf{Constraint Graph}.}

\begin{action}
    Clear a space. Draw 4 nodes in a diamond shape: p, q, x, y.
\end{action}

\speech{In this graph, nodes are variables. An edge from $q \to p$ means $pt(q) \subseteq pt(p)$.
The algorithm uses a \textbf{Worklist} approach:

1. We start with the base constraints (like $p = \&x$). We add $x$ to $pt(p)$.
2. If $pt(p)$ changes, we look at all outgoing edges from $p$.
3. We propagate the new info to the neighbors.
4. We repeat this until the sets stop changing—a \textbf{Fixed Point}.}

\speech{The complexity here is $O(N^3)$. Why? Because of those dynamic Load/Store rules. In the worst case, a change in one pointer set can trigger a ripple effect that adds edges to the graph, which triggers more propagation.}

\section{4. Steensgaard’s Analysis (Unification) (3 Minutes)}

\speech{Now, $O(N^3)$ is acceptable for small programs. But for something like the Linux Kernel (millions of lines), it is too slow.

This brings us to \textbf{Steensgaard’s Analysis}.
Steensgaard asked: ``What if we sacrifice precision for speed?''}

\begin{action}
    Write: ``Steensgaard: Unification ($=$) not Subset ($\subseteq$)''
\end{action}

\speech{Instead of allowing data to flow in one direction ($q \subseteq p$), Steensgaard forces the sets to be identical ($q = p$).
If we assign `p = q`, we treat them as the \textbf{same node} in the graph.}

\begin{action}
    Draw two circles.
    1. Andersen: A points to B.
    2. Steensgaard: A and B are merged into one blob.
\end{action}

\speech{This turns the problem from Graph Reachability into \textbf{Union-Find}.
We can simply merge the sets of abstract locations.
The complexity drops to \textbf{Almost Linear} ($O(N \alpha(N))$).

The downside? \textbf{False Positives}.
If `p` points to `x`, and `q` points to `y`, and we do `p = q`... Steensgaard merges `x` and `y`.
Now the analysis thinks `p` points to `y` (which might be true) but also that `q` points to `x` (which might be false).
We call this loss of precision.}

\section{5. Conclusion (1 Minute)}

\speech{To wrap up:
We looked at the problem of \textbf{Aliasing} and how it blocks optimization.
We explored \textbf{Andersen’s method}, which is precise and uses subset constraints, solved via a graph fixed-point algorithm.
And we contrasted it with \textbf{Steensgaard’s method}, which uses unification for extreme speed at the cost of precision.

In modern compilers like LLVM or GCC, we typically use inclusion-based analysis (Andersen's), but heavily optimized with techniques like \textbf{Cycle Detection} (collapsing cycles in the graph) to make it scalable.

Thank you, and I am open to questions.}

\newpage

\section*{Appendix: Exam Cheat Sheet}
\textit{(Print this page and keep it on the table for quick reference if you get stuck)}

\subsection*{1. The Motivation Example}
\begin{verbatim}
x = 10;
*p = 5;   // Does *p overwrite x?
y = x;    // Can we optimize y = 10?
\end{verbatim}

\subsection*{2. The 4 Rules (Andersen / Inclusion)}
\begin{itemize}
    \item \textbf{Alloc/Ref:} $p = \&x \implies \{x\} \subseteq pt(p)$
    \item \textbf{Assign:} $p = q \implies pt(q) \subseteq pt(p)$
    \item \textbf{Load:} $p = *q \implies \forall v \in pt(q): pt(v) \subseteq pt(p)$
    \item \textbf{Store:} $*p = q \implies \forall v \in pt(p): pt(q) \subseteq pt(v)$
\end{itemize}

\subsection*{3. Complexity Comparison}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Analysis} & \textbf{Constraint} & \textbf{Complexity} & \textbf{Data Structure} \\
\hline
Andersen & Subset ($\subseteq$) & $O(n^3)$ & Constraint Graph \\
\hline
Steensgaard & Equality ($=$) & $O(n \alpha(n))$ & Union-Find \\
\hline
\end{tabular}
\end{center}

\subsection*{4. Key Terminology}
\begin{itemize}
    \item \textbf{Abstract Location:} Represents a variable or a `malloc` site.
    \item \textbf{Flow-Insensitive:} The order of statements doesn't matter (sets accumulate).
    \item \textbf{Fixed Point:} When propagating sets produces no new changes.
    \item \textbf{Cycle Detection:} Optimization for Andersen's (all nodes in a cycle are merged).
\end{itemize}

\end{document}