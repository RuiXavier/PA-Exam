\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd} % For \llbracket and \rrbracket
\usepackage{xcolor}
\usepackage{fancyvrb}

% Formatting for readability
\onehalfspacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

% Custom commands
\newcommand{\speak}[1]{\textbf{\textcolor{blue}{#1}}} 
\newcommand{\note}[1]{\textit{\textcolor{gray}{[Note: #1]}}}
\newcommand{\sem}[1]{\llbracket #1 \rrbracket} % Shortcut for semantic brackets

\title{Oral Exam Script: Static Program Analysis}
\author{Candidate}
\date{\today}

\begin{document}

\maketitle

\section*{Topic 1: Type Analysis (Constraint-Based)}

\subsection*{1. The Elevator Pitch}
\speak{Definition:}
Type Analysis for TIP is a constraint-based static analysis that infers the types of variables and expressions in a program without executing it. Since TIP is not explicitly typed, we infer types to ensure consistency.

\speak{Why do we need it?}
We need it to guarantee \textbf{Type Safety}. We want to prevent runtime errors such as applying a non-function as a function, dereferencing an integer, or performing arithmetic on pointers.

\speak{Intuition:}
Imagine the AST as a puzzle. Each node has a "shape" (its type). We define rules for how pieces fit together (e.g., a dereference `*p` implies `p` must be a pointer shape). If we can fit all pieces together without force, the program is well-typed.

\subsection*{2. The Technical Core (Constraint Generation)}
\note{This uses the Constraint-Based template.}

\speak{The Type Lattice/Terms:}
Our terms $\tau$ include \texttt{int}, pointers $\uparrow \tau$, functions $(\tau_1 \dots) \rightarrow \tau$, and recursive types $\mu \alpha.\tau$.

\speak{Constraint Generation:}
We assign a type variable $\sem{E}$ to every expression node $E$. We generate equality constraints based on syntax:

\begin{itemize}
    \item \textbf{Allocations:} For `alloc E`, the result is a pointer to the type of E:
    $$ \sem{\text{alloc } E} = \uparrow \sem{E} $$
    \item \textbf{Dereference:} For `*E`, $E$ must be a pointer to the result:
    $$ \sem{E} = \uparrow \sem{*E} $$
    \item \textbf{Functions:} For a call $E(A_1)$, $E$ must be a function taking $A_1$ and returning the result:
    $$ \sem{E} = (\sem{A_1}) \rightarrow \sem{E(A_1)} $$
\end{itemize}

\speak{The Solving Mechanism (Unification):}
We solve these constraints using the \textbf{Union-Find} algorithm (Unification).
1. We iterate through constraints $t_1 = t_2$.
2. We find the canonical representatives of $t_1$ and $t_2$.
3. We \textbf{unify} them. If one is a type variable and the other is a proper type (like \texttt{int}), the variable adopts the proper type.
4. If we unify two incompatible proper types (e.g., \texttt{int} and $\uparrow \tau$), the program is \textbf{ill-typed}.

\subsection*{3. The Trace}
\speak{Code Snippet:}
\begin{verbatim}
var p, q;
p = alloc 10;
q = *p;
\end{verbatim}

\speak{Step-by-Step Trace:}
\begin{enumerate}
    \item \textbf{Generate Constraints:}
    \begin{itemize}
        \item From `alloc 10`: $\sem{\text{alloc } 10} = \uparrow \sem{10}$. We know $\sem{10} = \texttt{int}$, so $\sem{\text{alloc } 10} = \uparrow \texttt{int}$.
        \item From `p = ...`: $\sem{p} = \sem{\text{alloc } 10}$. Therefore, $\sem{p} = \uparrow \texttt{int}$.
        \item From `*p`: $\sem{p} = \uparrow \sem{*p}$.
        \item From `q = ...`: $\sem{q} = \sem{*p}$.
    \end{itemize}
    
    \item \textbf{Solve (Unification):}
    \begin{itemize}
        \item We know $\sem{p} = \uparrow \texttt{int}$.
        \item Substitute into the dereference constraint: $\uparrow \texttt{int} = \uparrow \sem{*p}$.
        \item Unify the inner types: $\texttt{int} = \sem{*p}$.
        \item Substitute into $q$: $\sem{q} = \texttt{int}$.
    \end{itemize}
    
    \item \textbf{Result:} Consistent. $p$ is a pointer to int, $q$ is an int.
\end{enumerate}

\subsection*{4. The Oral Exam Corner}
\speak{The Hook: Recursive Types}
"A fascinating detail is how we handle cycles during unification, like $X = \uparrow X$. Standard unification would fail (occurs check), but in TIP, we allow \textbf{Regular Types}. We introduce a recursive binder $\mu$, so the solution becomes $\mu \alpha . \uparrow \alpha$. This represents an infinite tree, allowing linked lists or self-referential structures."

\speak{The Trap: Polymorphism}
\textbf{Examiner:} "Does this analysis allow a function `id` to be used for both integers and pointers?"
\textbf{Answer:} "No, the standard analysis is \textbf{Monomorphic}. The function body is analyzed once. If `id` maps $\alpha \to \alpha$, and we unify $\alpha$ with \texttt{int} at the first call, a second call with a pointer will fail. To fix this, we would need \textbf{Let-Polymorphism}, which instantiates fresh type variables for every usage of the function."

\newpage

\section*{Topic 2: Monotone Frameworks (Dataflow)}

\subsection*{1. The Elevator Pitch}
\speak{Definition:}
The Monotone Framework is a general mathematical theory for dataflow analysis. Instead of creating ad-hoc algorithms for every analysis (Liveness, Reaching Definitions), we define a standard structure based on Lattices and Fixed Points.

\speak{Why do we need it?}
It provides a \textbf{guarantee of termination} and specific precision properties (Soundness) for any analysis that fits the framework constraints (monotonicity and finite height).

\speak{Intuition:}
Think of water flowing through a pipe network (the Control Flow Graph). The "water" is the information (e.g., which variables are live). The "pipes" are the instructions that might filter or add impurities (Transfer Functions). The "joints" are where pipes meet and water mixes (Join Operator).

\subsection*{2. The Technical Core (Monotone Framework)}
\note{This uses the Lattice/Dataflow template.}

\speak{1. The Lattice $(L, \sqsubseteq)$:}
We define a complete lattice $L$ representing abstract states.
\begin{itemize}
    \item $\bot$ (Bottom): Usually represents "no information" or "unreachable".
    \item $\sqsubseteq$ (Partial Order): Represents precision. $x \sqsubseteq y$ means $x$ is more precise (or safe) than $y$.
\end{itemize}

\speak{2. The Join Operator ($\sqcup$):}
This combines information from predecessor nodes (in forward analysis).
$$ \text{Input}(v) = \bigsqcup_{u \in pred(v)} \text{Output}(u) $$
If we want "Must" properties (Intersections), $\sqcup$ is $\cap$. If we want "May" properties (Unions), $\sqcup$ is $\cup$.

\speak{3. The Transfer Functions $f_\ell$:}
For every statement $\ell$, we define $f_\ell: L \rightarrow L$.
Typically defined as: $f_\ell(x) = (x \setminus \text{KILL}) \cup \text{GEN}$.
\textbf{Constraint:} $f$ must be \textbf{monotone}. If we know \textit{less} about the input, we should know \textit{less} (or the same) about the output.

\subsection*{3. The Trace (Sign Analysis)}
\speak{Code Snippet:}
\begin{verbatim}
var x;
x = 0;      // Line 1
x = x + 1;  // Line 2
\end{verbatim}

\speak{Lattice:} The Sign Lattice $\{ \bot, -, 0, +, \top \}$.
\speak{Trace:}
\begin{enumerate}
    \item \textbf{Init:} $x \mapsto \top$ everywhere (or $\bot$ if we optimize). Let's assume input to Line 1 is $\top$.
    \item \textbf{Line 1 (x = 0):}
    Transfer function $f_1$ sets $x$ to $0$.
    $$ \text{After Line 1}: \{ x \mapsto 0 \} $$
    \item \textbf{Line 2 (x = x + 1):}
    Input is $\{ x \mapsto 0 \}$.
    Transfer function for $+$ looks at abstract table: $0 \oplus + = +$.
    $$ \text{After Line 2}: \{ x \mapsto + \} $$
    \item \textbf{Fixed Point:} Further iterations do not change the state. We proved $x$ is positive.
\end{enumerate}

\subsection*{4. The Oral Exam Corner}
\speak{The Hook: Widening}
"The standard framework relies on the lattice having \textbf{Finite Height} to guarantee the worklist algorithm terminates. However, powerful analyses like \textbf{Interval Analysis} have infinite height lattices. To solve this, we use \textbf{Widening ($\nabla$)}. If we see a bound growing unstable ($[0,1] \to [0,2]$), we extrapolate immediately to infinity ($[0, \infty]$) to force termination, at the cost of precision."

\speak{The Trap: MOP vs. MFP}
\textbf{Examiner:} "Does this algorithm give the perfect answer?"
\textbf{Answer:} "Not necessarily. We compute the \textbf{Maximal Fixed Point (MFP)}. Ideally, we want the \textbf{Meet Over all Paths (MOP)}, which tracks every specific path. However, by Distributivity, if the transfer functions are distributive, MFP = MOP. If not (like in Constant Propagation), MFP is less precise but safe."

\newpage

\section*{Topic 3: Control Flow Analysis (CFA)}

\subsection*{1. The Elevator Pitch}
\speak{Definition:}
Control Flow Analysis (CFA) determines the control flow graph for programs where the call targets are not known staticallyâ€”specifically in the presence of \textbf{Function Pointers} (higher-order functions) or \textbf{Dynamic Dispatch}.

\speak{Why do we need it?}
Standard dataflow analysis assumes the CFG is already built. In TIP (or JS/Python), `x()` could call anything. We cannot build the CFG without dataflow (knowing what `x` is), and we can't do dataflow without the CFG. CFA solves both simultaneously.

\speak{Intuition:}
It is a "Chicken and Egg" problem. We model it as a flow graph where "tokens" (functions) flow into variables. When a function token flows into a "call site" bucket, we dynamically wire up the cables (edges) to that function's body.

\subsection*{2. The Technical Core (The Cubic Framework)}
\note{This uses the Constraint-Based template.}

\speak{Constraint Generation:}
We define tokens $\{ \text{id} \}$ and subset constraints $\subseteq$.
\begin{enumerate}
    \item \textbf{Simple Flow:} For $x = y$, we generate $\sem{y} \subseteq \sem{x}$.
    \item \textbf{Function Creation:} For `foo() {...}`, we generate $\{ \text{foo} \} \subseteq \sem{\text{foo}}$.
    \item \textbf{Conditional Constraints (The Key):}
    For a call site $c: E(A)$, we generate:
    $$ \forall f: \quad f \in \sem{E} \implies \left( \sem{A} \subseteq \sem{f_{arg}} \wedge \sem{f_{ret}} \subseteq \sem{c} \right) $$
\end{enumerate}

\speak{The Solving Mechanism (Graph Reachability):}
We use a graph where nodes are AST variables and edges are inclusions.
This is called the \textbf{Cubic Framework} ($O(n^3)$).
Algorithm:
1. Propagate tokens along existing edges.
2. If a token $f$ reaches a call site node $\sem{E}$, \textbf{add new edges} corresponding to the conditional constraint (Argument $\to$ Parameter, Return $\to$ Call).
3. Repeat until stable.

\subsection*{3. The Trace}
\speak{Code Snippet:}
\begin{verbatim}
var id, x;
id = (y) -> { return y; }; // Function f1
x = id(5);                 // Call Site c1
\end{verbatim}

\speak{Step-by-Step Trace:}
\begin{enumerate}
    \item \textbf{Initial Setup:}
    \begin{itemize}
        \item Function token $\{ f1 \}$ is created.
        \item Assignment: $\{ f1 \} \subseteq \sem{id}$.
    \end{itemize}
    \item \textbf{Propagation:}
    \begin{itemize}
        \item Token $\{ f1 \}$ flows into $\sem{id}$.
    \end{itemize}
    \item \textbf{Conditional Trigger (at c1):}
    \begin{itemize}
        \item Solver sees call `id(5)`. Checks $\sem{id}$.
        \item Finds $\{ f1 \}$. Trigger fires!
        \item \textbf{Add Edge (Args):} $\sem{5} \subseteq \sem{f1_y}$. (Abstract value of 5 flows to param y).
        \item \textbf{Add Edge (Ret):} $\sem{f1_{ret}} \subseteq \sem{x}$.
    \end{itemize}
    \item \textbf{Final Flow:}
    \begin{itemize}
        \item 5 flows to $y$. $y$ returns to $f1_{ret}$. $f1_{ret}$ flows to $x$.
        \item Result: $\sem{x}$ contains 5 (or Int).
    \end{itemize}
\end{enumerate}

\subsection*{4. The Oral Exam Corner}
\speak{The Hook: Context Sensitivity}
"The basic analysis I described is \textbf{0-CFA} (Context Insensitive). It merges all calls to a function. If we call `id` with an Int and later with a Pointer, 0-CFA says the parameter can be \textit{both}, and the return can be \textit{both}, causing spurious flows (Int flowing to the pointer call site). \textbf{k-CFA} solves this by tagging tokens with the last $k$ call sites (the call string), effectively separating the dataflow for different contexts."

\speak{The Trap: Complexity}
\textbf{Examiner:} "You mentioned Cubic Time. Why is it $O(n^3)$?"
\textbf{Answer:} "In the constraint graph, we have variables (nodes) and subset relations (edges). In the worst case, every variable points to every other variable ($n^2$ edges), and every function flows across every edge ($n$ functions). $n \times n^2 = O(n^3)$. This bottleneck is why scalable pointer analysis remains a significant challenge."

\end{document}